{"name":"Machine-learning","tagline":"Modeling HAR-Machine Learning Project","body":"# Modeling How Well Weight Lifting Exercise Is Performed with Human Activity Recognition\r\nWhile people regularly quantify how much of a particular activity they do, they rarely quantify how well it is performed. The data under consideration is associated with human activity recognition monitored by activity trackers on six male subjects, aged 20-28, located on the forearm, arm, belt, and the dumbbell used to perform the activity known as a biceps curl.\r\n\r\nIn this project, the goal is to use data from accelerometers located in activity trackers that aim to measure barbell lifts correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).\r\n\r\nTo begin, it's necessary to load the appropriate packages; in this case, the caret and randomForest package may be used:\r\n    library(caret)\r\n    library(randomForest)\r\n    library(RCurl)\r\n\r\n## Download Data\r\nThe data sets comprise a Weight Lifting Exercise Dataset available here: http://groupware.les.inf.puc-rio.br/har. For more information, see the sources at the end of this analysis.\r\n\r\n    # Download the data and read it\r\n    file <- getURL(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", ssl.verifypeer=0L, followlocation=1L)\r\n    writeLines(file, 'training.csv')\r\n    file <- read.csv(\"training.csv\")\r\n    testfile <- getURL(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", ssl.verifypeer=0L, followlocation=1L)\r\n    writeLines(testfile, 'testing.csv')\r\n    testfile <- read.csv(\"testing.csv\")\r\n\r\n## Data Cleaning\r\nLooking at the data, we can see that there are many columns with NA values or empty cells that need to be removed before analysis. In addition, there are several columns at the beginning of the data set that are irrelevant for the purposes of this analysis, and can also be removed. Removing these from the test set yields 53 variables with recorded observations; similarly, when we remove the columns with empty values in the training set, the column variables match with an equal 53 variables.\r\n\r\n    training <- file\r\n    training <- training[, colSums(is.na(training))==0]\r\n    training <- training[, -c(12:20, 43:48, 52:60, 74:82)]\r\n    training <- training[, c(8:60)]\r\n    testing <- testfile\r\n    testing <- testing[, colSums(is.na(testing))==0]\r\n    testing <- testing[, c(8:60)]\r\n\r\n## Cross Validation\r\nTo start cross validation, we take the training set, and split it into training and test sets. From there, we can build a model on the training set. The variable used for the split is the one which we want to predict of the test set, 'classe'. As we are using a random forest model, as described later, the approach is random sub-sampling with replacement (bootstrapping) which is estimated internally during the run, and gives an \"out-of-bag\" error estimate.\r\n    # Create training and test sets\r\n    inTrain <- createDataPartition(y = training$classe, p = 0.7, list = F)\r\n    train <- training[inTrain,]\r\n    test <- training[-inTrain,]\r\n    dim(train); dim(test)\r\n\r\n    # Plot of Quantitative Data by Class\r\n    qplot(classe, data = train, main = \"Data by Qualitative Class\", xlab = \"Qualitative Class\", ylab = \"Quantity\")\r\n\r\n### Build Model\r\nIn this case, we've selected the random forest model because of its accuracy, even though it may be subject to over-fitting and difficult to interpret at times. It is also among the top two performing algorithms, along with boosting, in prediction contests. In this model, the type of random forest is classification (as we are basing the movements performed by their class), the number of trees is 500, and the number of variables tried at each split is 7. The variable tested against the rest is 'classe'.\r\n    # Random forest\r\n    modFit <- randomForest(classe ~ ., data = train)\r\n    modFit\r\n    plot(modFit, log=\"y\")\r\n\r\n    # Predicting new values\r\n    pred <- predict(modFit, test)\r\n    confusionMatrix(pred, test$classe)\r\nAccording to the random forest model run here, the out of sample error is expected to be around 0.51%.\r\n\r\n### Model Prediction\r\nNow that the model has been tested on the training set and run on the testing data within that set, its possible to try it now on the separate testing data file (the validation data set), which had twenty observations of the same 53 variables.\r\n    # Predicted Result\r\n    predtest <- predict(modFit, testing)\r\n    predtest\r\n\r\n## Conclusion\r\nOverall, the random forest model appears to be highlight accurate in predicting the type of class of the activity performed (A-E) based on the human activity recognition data. However, it is possible that over-fitting on the training set may be a factor here.\r\n\r\n## Sources\r\n- Practical Machine Learning, Johns Hopkins University, Coursera\r\n\r\n- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. See more here: http://groupware.les.inf.puc-rio.br/har","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}